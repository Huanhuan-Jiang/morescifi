{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciFiConfig:\n",
    "    vocab_size: int = 100277  # cl100k-base\n",
    "    n_embd: int = 768  # GPT-2\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * config.n_embd, config.vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.uniform_(module.weight, -1.0, 1.0)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        #B, T = idx.shape\n",
    "        tok_emb = self.wte(idx) # (B, T, n_embd)\n",
    "        logits = self.proj(self.gelu(self.fc(tok_emb)))\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "    \n",
    "    def optimizer(self, learning_rate):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n",
    "        return optimizer\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "def load_tokens(filename):\n",
    "    tokens = np.loadtxt(filename, dtype=np.int32)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    return tokens\n",
    "\n",
    "class DataLoaderSciFi:\n",
    "    def __init__(self, B, T, split=None):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        \n",
    "        # get filename of dataset\n",
    "        self.data_dir = 'data'\n",
    "        self.tokens_filename = 'tokens.txt'\n",
    "        self.tokens_path = os.path.join(self.data_dir, self.tokens_filename)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tokens = load_tokens(self.tokens_path)\n",
    "        self.cur_pos = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.cur_pos : self.cur_pos + B * T + 1]\n",
    "        x = buf[: B * T].view(B, T)\n",
    "        y = buf[1: ].view(B, T)\n",
    "        self.cur_pos = self.cur_pos + B * T \n",
    "        return x, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# enc = tiktoken.get_encoding('cl100k_base')\n",
    "# text = \"Hello scientific fiction!\"\n",
    "# tokens = torch.tensor(enc.encode(text))\n",
    "# targets = torch.cat((tokens[1:], torch.tensor([-1])), dim=-1)\n",
    "\n",
    "# print(f\"Encoded text: {tokens}\")\n",
    "# print(f\"Targets: {targets}\")\n",
    "# print(f\"The vocab_size of cl100k_base is {enc.n_vocab}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "step 0 duration: 542.65ms loss=11.5170\n",
      "step 100 duration: 104.33ms loss=10.8140\n",
      "step 200 duration: 104.66ms loss=8.4314\n",
      "step 300 duration: 105.02ms loss=6.1556\n",
      "step 400 duration: 104.98ms loss=8.1009\n",
      "step 500 duration: 104.59ms loss=6.6083\n",
      "step 600 duration: 104.75ms loss=5.9500\n",
      "step 700 duration: 104.50ms loss=6.4254\n",
      "step 800 duration: 104.55ms loss=5.7874\n",
      "step 900 duration: 104.67ms loss=5.8936\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "B = 64\n",
    "T = 64\n",
    "max_steps = 6847 # 28046749 tokens / (B * T)\n",
    "max_lr = 3e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "\n",
    "def get_lr(step):\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "loader = DataLoaderSciFi(B=B, T=T)\n",
    "model = MLP(SciFiConfig)\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "optimizer = model.optimizer(learning_rate=2e-3)\n",
    "\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "for step in range(1000):\n",
    "    t0 = time.time()\n",
    "    x, y = loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f'step {step} duration: {dt*1000:.2f}ms loss={loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### duration for one step:\n",
    "B = 64 \\\n",
    "T = 64 \\\n",
    "max_steps = 6847 # 28046749 tokens / (B * T) \\\n",
    "max_lr = 3e-4 \\\n",
    "min_lr = max_lr * 0.1 \\\n",
    "warmup_steps = 715 \\\n",
    "\n",
    "initially: 190 ms \\\n",
    "with autocast to torch.bfloat16: 110 ms \\\n",
    "with torch.compile: 104 ms \\\n",
    "\n",
    "The lowest loss can be achieved is ~5.8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huanhuanjiang/global-envs/myenv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: In 2024, a boy was sitting in front of a park. I should\n",
      "stared, yes for one, I don't you can't know and looked about me, or not\n",
      "the outer his\n",
      "tremendous. Then\n",
      "are a heavy, while that to them, I could not know exactly than I asked.\n",
      "I'm going to the rest and I\n",
      "sh, but also at all who had already, and we shall be that our\n",
      "myself on his head for you to us, we have never know we do with his foot\n",
      "the sky and to find that I'll to see,\" said,\n",
      "sample 1: In 2024, a boy was sitting in front of a park. You'd. One as you know my own eyes\n",
      "for the same time or I came down. They were the long after a lot of that at my surprise's eyes were\n",
      "of the same time. I was gone\n",
      "in which a moment--not a large\n",
      "till at the ship, however, I've been, we were very\n",
      "reached. There is only man said, and they had not to the whole\n",
      "man, I had been so long ago, sir? Could. You know?\"\n",
      "\"Come that was\n",
      "man and\n",
      "the great\n",
      "sample 2: In 2024, a boy was sitting in front of a park. The world?'\n",
      ", however, it on the black. I did not know how from all\n",
      "are the man who was right, she heard his shoulder him, there. There was\n",
      "for their head and you can use to be better in\n",
      "of him, I stood the room and the\n",
      "and\n",
      "his the ship like a hundred thousand and\n",
      "were and was a few, or so much as a few.\"\n",
      "\"You never get\n",
      "down. You see to do with an opening, and his own.\n",
      "\"Good was more than it into it\n",
      "and I\n",
      "sample 3: In 2024, a boy was sitting in front of a park. You have done from it. A. I\n",
      "the same was\n",
      "me. In\n",
      "of the\n",
      "s.\n",
      "\"Exactly of the\n",
      "sweep, they'll get me\n",
      "not--when\n",
      "the earth.\n",
      "  *  * 25 and for all. It was it would send's been so, I don't\n",
      "the thing could not a man said.\"\n",
      "\"Why,\" answered. \"Look to the deck to know the sea; the way. The other than a minute that he turned off as it was to break and\n",
      "and\n",
      "had passed, though\n"
     ]
    }
   ],
   "source": [
    "# generate a sample after 1000 iterations\n",
    "import tiktoken\n",
    "\n",
    "max_length = 128\n",
    "num_samples = 4\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "tokens = enc.encode(\"In 2024, a boy was sitting in front of a park.\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (4, n_tokens)\n",
    "gen_idx = tokens.to(device)\n",
    "\n",
    "sample_rng = torch.Generator(device=device)\n",
    "sample_rng.manual_seed(2028)\n",
    "\n",
    "while gen_idx.size(1) < max_length:\n",
    "     logits, _ = model(gen_idx) # logits (num_samples, vocab_size)\n",
    "     logits = logits[:, -1, :]\n",
    "     probs = F.softmax(logits, dim=-1) \n",
    "     top_probs, top_indices = torch.topk(probs, 50, dim=-1)\n",
    "     ix = torch.multinomial(top_probs, 1, generator=sample_rng)\n",
    "     out_ix = torch.gather(top_indices, -1, ix)\n",
    "     gen_idx = torch.cat((gen_idx, out_ix), dim=1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "     tokens = gen_idx[i, :].tolist()\n",
    "     decoded = enc.decode(tokens)\n",
    "     print(f\"sample {i}: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
