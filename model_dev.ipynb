{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciFiConfig:\n",
    "    vocab_size: int = 100277  # cl100k-base\n",
    "    n_embd: int = 768  # GPT-2\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * config.n_embd, config.vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.uniform_(module.weight, -1.0, 1.0)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        #B, T = idx.shape\n",
    "        tok_emb = self.wte(idx) # (B, T, n_embd)\n",
    "        logits = self.proj(self.gelu(self.fc(tok_emb)))\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "    \n",
    "    def optimizer(self, learning_rate):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n",
    "        return optimizer\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "def load_tokens(filename):\n",
    "    tokens = np.loadtxt(filename, dtype=np.int32)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    return tokens\n",
    "\n",
    "class DataLoaderSciFi:\n",
    "    def __init__(self, B, T, split=None):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        \n",
    "        # get filename of dataset\n",
    "        self.data_dir = 'data'\n",
    "        self.tokens_filename = 'tokens.txt'\n",
    "        self.tokens_path = os.path.join(self.data_dir, self.tokens_filename)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tokens = load_tokens(self.tokens_path)\n",
    "        self.cur_pos = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.cur_pos : self.cur_pos + B * T + 1]\n",
    "        x = buf[: B * T].view(B, T)\n",
    "        y = buf[1: ].view(B, T)\n",
    "        self.cur_pos = self.cur_pos + B * T \n",
    "        return x, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# enc = tiktoken.get_encoding('cl100k_base')\n",
    "# text = \"Hello scientific fiction!\"\n",
    "# tokens = torch.tensor(enc.encode(text))\n",
    "# targets = torch.cat((tokens[1:], torch.tensor([-1])), dim=-1)\n",
    "\n",
    "# print(f\"Encoded text: {tokens}\")\n",
    "# print(f\"Targets: {targets}\")\n",
    "# print(f\"The vocab_size of cl100k_base is {enc.n_vocab}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "step 0 duration: 14.29ms loss=11.5189\n",
      "step 100 duration: 193.06ms loss=6.5283\n",
      "step 200 duration: 197.85ms loss=5.3500\n",
      "step 300 duration: 195.67ms loss=4.8795\n",
      "step 400 duration: 197.31ms loss=8.5506\n",
      "step 500 duration: 196.71ms loss=6.2702\n",
      "step 600 duration: 196.45ms loss=5.4567\n",
      "step 700 duration: 196.80ms loss=5.9329\n",
      "step 800 duration: 197.70ms loss=5.6451\n",
      "step 900 duration: 194.07ms loss=5.8578\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "B = 64\n",
    "T = 64\n",
    "max_steps = 438230 # 28046749 tokens / (B * T)\n",
    "mini_steps = 1000\n",
    "lr = 3e-3\n",
    "\n",
    "loader = DataLoaderSciFi(B=B, T=T)\n",
    "model = MLP(SciFiConfig)\n",
    "model.to(device)\n",
    "optimizer = model.optimizer(lr)\n",
    "\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "for step in range(mini_steps):\n",
    "    t0 = time.time()\n",
    "    x, y = loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f'step {step} duration: {dt*1000:.2f}ms loss={loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: In 2024, a boy was sitting in front of a park. I would\n",
      "cunning at what might you are you will be\n",
      "that he\n",
      "me_ be\n",
      "as he could not\n",
      "for each of this time the\n",
      "and there is.\"\n",
      "\"You'd better yet in hand. \"I don't think in upon\n",
      "had failed.\"\n",
      "\"Sure. It seemed to be in in this very\n",
      "her, after all over. I've had been\n",
      "their minds from the\n",
      "n't been the top of the\n",
      "succeeded in an intelligent faces.\"\n",
      "\"Where do, the shipwonderful\n",
      "not to make you,\n",
      "sample 1: In 2024, a boy was sitting in front of a park. He moved in so the\n",
      "to die, the girl with the first to call\n",
      "be to their position. \"Well the\n",
      "his\n",
      "you didn't hurt.\n",
      "\"But why it, for\n",
      "for you, even for it\n",
      "all to the rest of course.\n",
      "\"Exactly.\" She can't see. He\n",
      "was, for I'd never have\n",
      "the worst.\n",
      "\"If you and more\n",
      "of men. It's a week.\"\n",
      "\"I'm not know how\n",
      "paled, not of their hands.\n",
      "I see your\n",
      "you're going to a\n",
      "the first\n",
      "sample 2: In 2024, a boy was sitting in front of a park. We will get back to see you\n",
      "their winger of\n",
      "her, but that,\n",
      "it's heart of his feet and her in front\n",
      "tastes; but so far to me, not do? I would be one, and\n",
      "on, I asked. \"Not_ like it was no more or it, in what you know how many times.\n",
      "\"Oh. \"It might be more. I was a week. After,\" she heard, for the water, it?\"\n",
      "\"There's a thing, \"I'm just that\n",
      "was going to do. \"\n",
      "sample 3: In 2024, a boy was sitting in front of a park. He\n",
      "were going to be very little, though, I will not the ground, though. \"I do anything about. The most of the old. And in all three hours ago.\"\n",
      "\"So do you,\" said; now,\" my own experience.\n",
      "\"To my life.\n",
      "He caught and what had told us. \"And very\n",
      "the girl.\" She might be now she is my face. It is to the water--no lawless man in its\n",
      "s and in\n",
      "were only by his teeth. She did you going to\n",
      "a of\n",
      "beings would\n"
     ]
    }
   ],
   "source": [
    "# generate a sample after 1000 iterations\n",
    "import tiktoken\n",
    "\n",
    "max_length = 128\n",
    "num_samples = 4\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "tokens = enc.encode(\"In 2024, a boy was sitting in front of a park.\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_samples, 1) # (4, n_tokens)\n",
    "gen_idx = tokens.to(device)\n",
    "\n",
    "sample_rng = torch.Generator(device=device)\n",
    "sample_rng.manual_seed(2028)\n",
    "\n",
    "while gen_idx.size(1) < max_length:\n",
    "     logits, _ = model(gen_idx) # logits (num_samples, vocab_size)\n",
    "     logits = logits[:, -1, :]\n",
    "     probs = F.softmax(logits, dim=-1) \n",
    "     top_probs, top_indices = torch.topk(probs, 50, dim=-1)\n",
    "     ix = torch.multinomial(top_probs, 1, generator=sample_rng)\n",
    "     out_ix = torch.gather(top_indices, -1, ix)\n",
    "     gen_idx = torch.cat((gen_idx, out_ix), dim=1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "     tokens = gen_idx[i, :].tolist()\n",
    "     decoded = enc.decode(tokens)\n",
    "     print(f\"sample {i}: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
