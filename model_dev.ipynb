{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import math\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciFiConfig:\n",
    "    vocab_size: int = 100277  # cl100k-base\n",
    "    n_embd: int = 768  # GPT-2\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    \n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.proj = nn.Linear(4 * config.n_embd, config.vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.uniform_(module.weight, -1.0, 1.0)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        #B, T = idx.shape\n",
    "        tok_emb = self.wte(idx) # (B, T, n_embd)\n",
    "        logits = self.proj(self.gelu(self.fc(tok_emb)))\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: tensor([ 9906, 12624, 17422,     0])\n",
      "Targets: tensor([12624, 17422,     0,    -1])\n",
      "The vocab_size of cl100k_base is 100277.\n"
     ]
    }
   ],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# enc = tiktoken.get_encoding('cl100k_base')\n",
    "# text = \"Hello scientific fiction!\"\n",
    "# tokens = torch.tensor(enc.encode(text))\n",
    "# targets = torch.cat((tokens[1:], torch.tensor([-1])), dim=-1)\n",
    "\n",
    "# print(f\"Encoded text: {tokens}\")\n",
    "# print(f\"Targets: {targets}\")\n",
    "# print(f\"The vocab_size of cl100k_base is {enc.n_vocab}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are len(tokens) tokens in scifi dataset.\n",
      "[791, 4212, 13257, 11, 555, 473, 13, 480, 13, 37958, 510, 9378, 23, 933, 40, 198, 791, 4212, 43359, 7218, 320, 2000, 779, 433, 690, 387, 17125, 311, 6604, 315, 1461, 340, 16514, 1367, 13900, 264, 312, 1321, 635, 5030, 311, 603, 13, 5414, 20366, 6548, 559, 606, 323, 198, 15930, 771, 839, 11, 323, 813, 6118, 28639, 3663, 574, 74820, 323, 11625, 13, 578, 198, 11029, 27724, 76389, 11, 323, 279, 8579, 12164, 685, 315, 279, 3709, 62452, 1189, 198, 14146, 304, 279, 326, 7751, 315, 15310, 10791, 279, 44783, 430, 70939, 323, 198, 36522, 304, 1057, 29247, 13]\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "tokens_filename = 'tokens.txt'\n",
    "tokens_path = os.path.join(data_dir, tokens_filename)\n",
    "\n",
    "with open(tokens_path, 'r') as f:\n",
    "    tokens = f.read()\n",
    "    tokens = list(map(int, tokens.split()))\n",
    "print(f'There are len(tokens) tokens in scifi dataset.')\n",
    "# print out a few tokens\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  791,  4212, 13257,    11,   555,   473,    13,   480,    13, 37958,\n",
      "          510,  9378,    23,   933,    40,   198,   791,  4212, 43359,  7218,\n",
      "          320,  2000,   779,   433,   690,   387, 17125,   311,  6604,   315,\n",
      "         1461,   340, 16514,  1367, 13900,   264,   312,  1321,   635,  5030,\n",
      "          311,   603,    13,  5414, 20366,  6548,   559,   606,   323,   198,\n",
      "        15930,   771,   839,    11,   323,   813,  6118, 28639,  3663,   574,\n",
      "        74820,   323, 11625,    13,   578,   198, 11029, 27724, 76389,    11,\n",
      "          323,   279,  8579, 12164,   685,   315,   279,  3709, 62452,  1189,\n",
      "          198, 14146,   304,   279,   326,  7751,   315, 15310, 10791,   279,\n",
      "        44783,   430, 70939,   323,   198, 36522,   304,  1057, 29247,    13])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243077/3212287203.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens = torch.tensor(tokens)\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor(tokens)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of pred: torch.Size([4, 25, 100277]).\n",
      "loss=11.5135\n"
     ]
    }
   ],
   "source": [
    "model = MLP(SciFiConfig)\n",
    "B = 4\n",
    "T = 25\n",
    "idx = tokens[:100].view(B, T)\n",
    "targets = tokens[1:101].view(B, T)\n",
    "logits, loss = model(idx, targets)\n",
    "print(f'The shape of pred: {logits.shape}.')\n",
    "print(f'loss={loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
